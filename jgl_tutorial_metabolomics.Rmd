---
title: "Joint Graphical Lasso Tutorial"
knit: (function(input_file, encoding) {
    out_dir <- 'docs';
    rmarkdown::render(input_file,
      encoding=encoding,
      output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
subtitle: "Presentation for UMass Amherst PhD Biostat Seminar 892a"
author: "Margaret Janiczek, Ariane Stark"
date: "4/1/2022"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float: true
    code_folding: hide 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

This tutorial will summarize details of the Joint Graphical Lasso (JGL) algorithm (Danaher et. al 2011), then walk through the steps needed to run the `JGL` package using a real-world metabolomics dataset. 

Code throughout the document is "hidden" but if you click on "CODE" in the upper right above any R output, you can expand it to view code and (extensive) comments!

All code is available on my [GitHub](github.com/mljaniczek). Please see this [link]([github.com/starkari/JGL-S22-Presentation) for Ariane Stark's companion presentation which contains further elaboration on results presented in the Danaher paper.

## Acknowledgements

Thank you to Raji Balasubramanian for the metabolomics data, which was previously presented at ENAR 2021.

Thank you to Kate Shutta and Yukun Li who provided code for the beautiful circular network graphs. 

# Joint Graphical Lasso Overview

## Background

* We previously discussed details of graphical lasso to estimate precision matrix for Gaussian Graphical Models (GGMs) as in Yuan and Lin (2007) and Friedman et al (2007).  

* Joint graphical lasso builds upon this by estimating *multiple, related GGMs* from data with observations belonging to distinct classes (for example, cancer vs normal tissue). 

* There are two types of penalty functions we can use within JGL which will adjust how much information to leverage across the classes. 

* Additionally, the algorithm uses a fast alternating directions method of multipliers. 

* Danaher paper has almost 800 citations as of March 2022.

## Notation

* $K$ number of classes ($\geq 2$).  Index classes using $k$ = 1, ... $K$. 
* $\Sigma^{-1}_k$: True precision matrix for the kth class 
* $Y^{(k)}$: $n_k$ x $p$ matrix consisting of $n_k$ observations from the $k$th class on a set of $p$ features which are common to all $K$ datasets  
* $S^{(k)}$: Empirical covariance matrix for $Y^{(k)}$  
* $\Theta^{(k)}$: argument to convex optimization problem used for estimating $\Sigma^{-1}_k$  
* Index matrix arguments by using $i$ = 1, ..., $p$ and $j$ = 1, ..., $p$  
* $\lambda_1$ and $\lambda_2$: non-negative tuning parameters used in penalty function

## Major assumptions

* We assume the observations **within** each class are iid. 
* Also assume $\mu_k$, the mean for each class, is 0. i.e:   

$$Y^{(k)}_1, ..., Y^{(k)}_{nk} \sim N(0, \Sigma_k)$$

## Optimization problem

* Our goal is to estimate $\Sigma^{-1}_1$, ..., $\Sigma^{-1}_K$ by using penalized log-likelihood approach. 

* Seek {$\hat{\Theta}$} by solving:

$$maximize_{\{\Theta\}}\left(\sum^{K}_{k=1}n_k[log\{det(\theta^{(k)}\}-tr(S^{(k)}\Theta^{(k)}) - P(\{\Theta\})\right)$$

* A **major innovation of this paper**, is the generalization of the optimization problem to multiple classes, in addition to using the penalty function $P(\{\Theta\})$, for which the authors provide two different versions. 

## Penalty functions

### Fused Graphical Lasso

* Fused Graphical Lasso (FGL) uses the following penalty function:

$$P(\{\Theta\}) = \lambda_1\sum^K_{k=1}\sum_{i \neq j}|\theta^{(k)}_{ij}| + \lambda_2\sum_{k<k'}\sum_{i,j}|\theta^{(k)}_{ij}-\theta^{(k')}_{ij}|$$

* When $\lambda_1$ is **large**, FGL makes sparse estimates of $\hat{\Theta}^{(1)}, ... , \hat{\Theta}^{(K)}$ 
* When $\lambda_2$ is **large**, many elements of $\hat{\Theta}^{(1)}, ... , \hat{\Theta}^{(K)}$ will be the same across classes  
* So, FGL "borrows information aggressively across classes, encouraging similar network structure and similar edge values"

### Group Graphical Lasso

* Group Graphical Lasso (GGL) uses the following penalty function:

$$P(\{\Theta\}) = \lambda_1\sum^K_{k=1}\sum_{i \neq j}|\theta^{(k)}_{ij}| + \lambda_2\sum_{i \neq j}\left(\sum_{i,j}{\theta^{(k)}_{ij}}^2\right)^{1/2}$$

* Lasso penalty applied to elements of the precision matrices  
* Group lasso penalty is applied to the (i, j) element across all K precision matrices
* When $\lambda_1$ is **large**, GGL makes sparse estimates of $\hat{\Theta}^{(1)}, ... , \hat{\Theta}^{(K)}$ 
* So, GGL just encourages a shared pattern of *sparsity*, not shared *edge values* (unlike FGL which encourage sharing across both)



# Application of JGL to metabolomics dataset

## Data Processing

Data comes from Hyperglycemia and Adverse Pregnancy Outcome (HAPO) Study. Metabolites were measured from blood samples taken from pregnant women from four ancestry groups: Afro-Caribbean, Mexican American, Northern European, and Thai. 

The data consists of ID variable, ancestry group, and metabolites (p = 51 features). The ancestry groups (K = 4) are balanced (n = 400 each, with 1600 total observations).There is also a variable "fpg" which is fasting plasma glucose, which I omit from this analysis. 

Within the data there are three groups of metabolites: Amino Acids, Acycl carnitines, and Other.

```{r load_data_packages, message = FALSE, warning=FALSE, include=FALSE}
#install.packages("JGL")
#install.packages("JointNets")
#install.packages("tidyverse")

library(JGL)
library(JointNets) # for plotting JGL results 
library(igraph)
library(tidyverse)
library(igraph)
library(RColorBrewer)
library(pheatmap)
source(here::here("helper_functions.r")) #contains functions I made using code from Kate and Yukun and one function from Koyeje lab GitHub
theme_set(theme_bw()) #setting ggplot theme 

# read in metabolomics data from Raji
dat <- read.csv(here::here("data/hapo_metabolomics_2020.csv"))
```

```{r}
# examine ancestry group variable
summary(as.factor(dat$anc_gp))
```

### Center and Scale Data within Class

We can see that the raw data has different distributions, but assumptions of JGL need data to be centered and scaled. 

```{r}
head(dat)[,1:10]

summary(dat$mt1_1)
summary(dat$mt2_1)
summary(dat$mt3_1)

```

Center and scale each metabolite feature, to satisfy assumptions of JGL. 


```{r data_long}

# prepare data into list of K datasets as needed as input to JGL function

# make data into long format then center and scale by metabolite

## we do not want to center and scale within each group because that would remove any signal we see, right? 

dat_long <- dat %>%
  select(-fpg) %>%
  pivot_longer(cols = starts_with("mt"),
               names_to = "metabolite",
               values_to = "value",
               values_drop_na = TRUE) %>%
  group_by(metabolite) %>%
  mutate(scaled_value = scale(value, center = TRUE, scale = TRUE)) %>%
  ungroup()

head(dat_long)
```


```{r density_plots}
# if you want to look at one metabolite
# dat_long %>% 
#   filter(metabolite %in% c("mt1_1")) %>%
#   ggplot(aes(x=value, color=anc_gp)) +
#     geom_density()+
#   facet_grid(metabolite ~.)

# demonstrative density plot showing centering/scaling of data
dat_long %>% 
  filter(metabolite %in% c("mt1_1", "mt2_1", "mt3_1")) %>%
  ggplot(aes(x=scaled_value, color=anc_gp)) +
    geom_density()+
  facet_grid(metabolite ~.)

```

### Make *K* *n* by *p* matrices

Now the raw data is centered and scaled! But first we need to get it into *K* *n* by *p* matrices as required by the `JGL` package. 

So in our case, we need **4** matrices (4 ancestry groups), each with **400** observations of **51** metabolites. 

*Note: Normally we would impute any missing data. For simplicity in this code I treated missing data as "0" value.*

```{r}
# make list of ancestry groups
ancestry_groups <- sort(unique(dat$anc_gp))

# filter by ancestry group, then make wide matrix of scaled values
# create a list of the results
# Here I'm using the `purrr::map` to iterate over the 4 ancestry groups and make the list. You could use base R lapply instead if you prefer.
dat_mat_list <- purrr::map(ancestry_groups,
                    ~dat_long %>%
                      filter(anc_gp == .x) %>%
                      select(-c(anc_gp, value)) %>%
                      pivot_wider(names_from = metabolite, 
                                  values_from = scaled_value,
                                  #for simplicity putting 0 as missing values. Normally should do imputation.
                                  values_fill = 0) %>%
                      select(-id) %>%
                      as.matrix())

names(dat_mat_list) <- ancestry_groups
```

Let's check to see if our list contains the right structure: 

```{r}
str(dat_mat_list)
```

It does! So now we have a list of 4 matrices, each with 400 standardized observations of 51 metabolites. 

### Heatmap to visualize the processed data

Before we proceed, let's visualize the processed data quick in a heatmap to get an idea of if the data has any patterns. 

```{r heatmap}

# making one full wide data frame with all 1600 observations, using the centered-scaled values for metabolites that we processed above. 
met_wide_all <- dat_long %>%
  select(-value) %>%
  pivot_wider(names_from = metabolite, 
              values_from = scaled_value,
              #for simplicity putting 0 as missing values. Normally should do imputation.
              values_fill = 0) %>%
  arrange(anc_gp) #arranging by ancestry group for the heatmap

#make matrix out of dataframe, removing the id and group labels since pheatmap just takes numeric matrix as input
met_wide_all_mat <- t(as.matrix(met_wide_all%>%
  select(-c(id, anc_gp))))
met_wide_all_mat[is.na(met_wide_all_mat)] <- 0
colnames(met_wide_all_mat) <- met_wide_all$id

#pheatmap can annotate your heatmap nicely with colors per class! so I'm preparing those here. 

#first I want the columns annotated by ancestry group
my_sample_col <- data.frame(ancestry = as.factor(met_wide_all$anc_gp))
rownames(my_sample_col) <- met_wide_all$id
# then I want the rows annotated by metabolite group, we have three groups of metabolites mt1, mt2, mt3
met_group <- str_split_fixed(rownames(met_wide_all_mat), "_", 2)[,1] %>%
  str_replace_all(c("mt1" = "Amino Acids",
                  "mt2" = "Acyl carnitines",
                  "mt3" = "Other"))
my_sample_row <- data.frame(met_group = met_group)
row.names(my_sample_row) <- row.names(met_wide_all_mat)

# finally we input the numeric matrix and annotations
pheatmap(met_wide_all_mat, 
         cluster_cols = FALSE, 
         show_colnames = FALSE, 
         show_rownames = FALSE, 
         annotation_col = my_sample_col,
         annotation_row = my_sample_row)
```

Here the x axis has the 1600 samples, sorted by ancestry group. 
The y axis has the 51 metabolites, clustered using hierarchical clustering, and with a color label based on metabolite group (there are 3 "groups" in the data, with suffix mt1, mt2, and mt3 which indicate Amino Acids, Acyl carnitines, and other groups respectively). 

From this heatmap we can see there are big differences in the values for "Other" metabolite group (pink) across the 4 ancestry groups. metabolites from Amino Acids and Acyl Carnitines groups appear to be more homogeneous across the classes.

Let's get to joint graphical lasso now! We will essentially be taking the data from each ancestry section and fitting graphical lasso, while also borrowing strength from the other classes which is a strength of the JGL. 

## Apply Joint Graphical Lasso

Danaher et. al wrote the R package `JGL` as companion to their paper. There are two penalty functions for `lambda2` described in the paper: **Fused Graphical Lasso (FGL)** and **Group Graphical Lasso (GGL)** penalties.

We will first go over some basic results using preset values for lambda1 and lambda2 parameters, visualize the results, then discuss strategies for tuning the parameters.  


### Using Fused Graphical Lasso penalty {.tabset}

We use the `JGL()` function from the `JGL` package, with `penalty = "fused"` specified. 

For now we are just putting in a preselected value for lambda1 and lambda2, but in a later section we will go over methods to tune the parameters. 

```{r run_fgl}
fgl_results = JGL(Y = dat_mat_list,
                  penalty = "fused",
                  lambda1 = .15,
                  lambda2 = 0.2,
                  return.whole.theta = FALSE)

str(fgl_results) # the theta contains a list of estimated matrices, one for each of the K classes. We will extract the thetas for visualization with igraph. 
print.jgl(fgl_results)
```

The basic code itself is simple, although the package does not come with easy-to-use visualization functions. The following contains a frankenstein mix of code I borrowed from online and from colleagues (thank you Kate and Yukun again!!). 

* Extract the estimated covariance matrices from the `JGL()` result  
* Use `graph_from_adjacency_matrix()` function from the `igraph` package on each of the K estimated covariance matrices  
* Either use `plot.igraph()` and tweak inputs to make a nice graph, or you can use the cobbled-together code that was passed down from Kate to Yukun to me, each of us has tweeked it slightly to how we like it! I have made the function `plot_jgl()` in the companion script `visualize.r` which is available in my GitHub along with this presentation.  

By examining the below graphs we can see that the estimated networks, especially for the strong correlations, is largely similar across the four groups. 

```{r visualize_fgl}
# extract all estimated covariance matrices from result
inv_covar_matrices <- fgl_results$theta
names(inv_covar_matrices) <- ancestry_groups

#now use `graph_from_adjacency_matrix()` function from igraph to create igraph graphs from adjacency matrices

#again since we have a list covariance matrices I use the map() function to apply igraph::graph_from_adjacency_matrix() over the list

graph_list <- map(ancestry_groups,
                  ~graph_from_adjacency_matrix(
                    -cov2cor(inv_covar_matrices[[.x]]),
                    weighted = T,
                    mode = "undirected",
                    diag = FALSE
                  ))
names(graph_list) <- ancestry_groups

# can use the basic igraph code to plot
# plot.igraph(graph_list[["ag1"]],
#             layout = layout.fruchterman.reingold)

# or can use funcion I made from Kate/Yukun's code to make a pretty circular graph
#plot_jgl(graph_list[[1]], multiplier = 3)

```

#### FGL Class 1

```{r}

plot_jgl(graph_list[[1]], multiplier = 3)
```

#### FGL Class 2

```{r}
plot_jgl(graph_list[[2]], multiplier = 3)
```

#### FGL Class 3

```{r}
plot_jgl(graph_list[[3]], multiplier = 3)
```

#### FGL Class 4

```{r}
plot_jgl(graph_list[[4]], multiplier = 3)

```



### Using Group Graphical Lasso penalty {.tabset}

Now run "group" penalty instead in `JGL()` function. Same procedure as above except with `penalty = "group"` as input. 

```{r run_ggl}
## run ggl:
ggl_results = JGL(Y=dat_mat_list,
                  penalty="group",
                  lambda1=.15,
                  lambda2=.2,
                  return.whole.theta=TRUE)
str(ggl_results)
print.jgl(ggl_results)
```

Extract estimated covariance matrices and prepare for visualization.

```{r visualize_ggl}
# extract all estimated covariance matrices from result
ggl_inv_covar_matrices <- ggl_results$theta
names(ggl_inv_covar_matrices) <- ancestry_groups

#now use function from igraph to create igraph graphs from adjacency matrices

ggl_graph_list <- map(ancestry_groups,
                  ~graph_from_adjacency_matrix(
                    -cov2cor(ggl_inv_covar_matrices[[.x]]),
                    weighted = T,
                    mode = "undirected",
                    diag = FALSE
                  ))
names(ggl_graph_list) <- ancestry_groups

```



#### GGL Class 1

```{r}

plot_jgl(ggl_graph_list[[1]], multiplier = 3)
```

#### GGL Class 2

```{r}
plot_jgl(ggl_graph_list[[2]], multiplier = 3)
```

#### GGL Class 3

```{r}
plot_jgl(ggl_graph_list[[3]], multiplier = 3)
```

#### GGL Class 4

```{r}
plot_jgl(ggl_graph_list[[4]], multiplier = 3)

```

BUT what we've done so far we've had to pre-select the lambdas. 

Let's show how we would input an assortment of lambdas and tune those parameters. 


## Tuning parameters

I found the [Koyejo Lab GitHub repository](https://github.com/koyejo-lab/JointGraphicalLasso) full of code to assist with different methods to select parameters for JGL. 

The below chunk is one example where they use the following process:

* Do a 2D grid search over a sequence of possible values for lambda1 and lambda2, run `JGL()` for each  
* Collect Akaike Information Criterion (AIC) for each combination  
* Identify lambda combination with the lowest AIC  
* Built final model with tuned parameters. 

You can use this method with both fused or group penalty. 

### Example tuning lambdas with metabolomics data using GGL

Below I'm making a very small 3x3 grid of lambdas for run-time demonstration purposes (it takes my computer less than a minute, but timing went up substantially when I added a larger grid). 

```{r}
interval_l = 3
lambda.eff <- seq(0.01, 0.3, len = interval_l)
aic_vec <- matrix(NA, length(lambda.eff),length(lambda.eff))

#search grid of lambdas
for(i in 1:length(lambda.eff)){
    for(j in 1:length(lambda.eff)){
        fit00 <- JGL(Y=dat_mat_list,penalty="group",lambda1=lambda.eff[i],lambda2=lambda.eff[j], return.whole.theta=TRUE)
        aic_vec[i,j] <- vBIC(dat_mat_list, fit00, thr=0.0001)
   }
}


min(aic_vec)
which(aic_vec == min(aic_vec), arr.ind = TRUE)

```


```{r visualize_tuning}

image(x=lambda.eff,y=lambda.eff,z=aic_vec,
      ylab='lambda_2',xlab='lambda_1')
contour(x=lambda.eff,y=lambda.eff,z=aic_vec, add = TRUE,nlevels = 10)
```

Identify lambda combination with lowest AIC.

```{r tuned_lambda}
# identify which had min aic
i_idx <- which(aic_vec == min(aic_vec), arr.ind = TRUE)[1,1]
j_idx <- which(aic_vec == min(aic_vec), arr.ind = TRUE)[1,2]

#assign tuned lambdas based on your search
lam_1 <- lambda.eff[i_idx]
lam_2 <- lambda.eff[j_idx]

```

### Run JGL again with tuned parameters {.tabset}

```{r}
print(paste("lambda_1", lam_1))
print(paste("lambda_2", lam_2))

ggl_tuned_results <- JGL(Y=dat_mat_list,
                  penalty="group",
                  lambda1= lam_1,
                  lambda2= lam_2,
                  return.whole.theta=TRUE)

# extract all estimated covariance matrices from result
inv_covar_matrices <- ggl_tuned_results$theta
names(inv_covar_matrices) <- ancestry_groups

#now use `graph_from_adjacency_matrix()` function from igraph to create igraph graphs from adjacency matrices

#again since we have a list covariance matrices I use the map() function to apply igraph::graph_from_adjacency_matrix() over the list

graph_list <- map(ancestry_groups,
                  ~graph_from_adjacency_matrix(
                    -cov2cor(inv_covar_matrices[[.x]]),
                    weighted = T,
                    mode = "undirected",
                    diag = FALSE
                  ))
names(graph_list) <- ancestry_groups
```

#### Tuned GGL Class 1

```{r}

plot_jgl(graph_list[[1]], multiplier = 3)
```

#### Tuned GGL Class 2

```{r}
plot_jgl(graph_list[[2]], multiplier = 3)
```

#### Tuned GGL Class 3

```{r}
plot_jgl(graph_list[[3]], multiplier = 3)
```

#### Tuned GGL Class 4

```{r}
plot_jgl(graph_list[[4]], multiplier = 3)

```





# References

Patrick Danaher, Pei Wang and Daniela Witten (2011). The joint graphical lasso for inverse covariance estimation across multiple classes. http://arxiv.org/abs/1111.0324


